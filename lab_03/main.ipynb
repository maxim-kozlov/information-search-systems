{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f658a00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.utils import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea83a7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import torch\n",
    "DEVICE = torch.device(\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6eb405f9-1510-489b-bdc0-7ccb3b659a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "!export PYTORCH_ENABLE_MPS_FALLBACK=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "677b902e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загружаем модель ruGPT от сбера\n",
    "model_name_or_path = \"../huggingface/hub/rugpt3large_based_on_gpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name_or_path)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name_or_path).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49e23846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mREADME.md\u001b[m\u001b[m               \u001b[31mmerges.txt\u001b[m\u001b[m              \u001b[31mtokenizer_config.json\u001b[m\u001b[m\n",
      "\u001b[31mconfig.json\u001b[m\u001b[m             \u001b[31mpytorch_model.bin\u001b[m\u001b[m       \u001b[31mvocab.json\u001b[m\u001b[m\n",
      "\u001b[31mflax_model.msgpack\u001b[m\u001b[m      \u001b[31mspecial_tokens_map.json\u001b[m\u001b[m\n"
     ]
    }
   ],
   "source": [
    "!ls \"../huggingface/hub/rugpt3large_based_on_gpt2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "705a20c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/transformers/generation/utils.py:1141: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.11/site-packages/transformers/generation/stopping_criteria.py:149: UserWarning: The operator 'aten::isin.Tensor_Tensor_out' is not currently supported on the MPS backend and will fall back to run on the CPU. This may have performance implications. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/mps/MPSFallback.mm:13.)\n",
      "  is_done = torch.isin(input_ids[:, -1], self.eos_token_id.to(input_ids.device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Вопрос: 'Сколько будет 2+2?' \n",
      " Ответ: `2+2=4\n"
     ]
    }
   ],
   "source": [
    "# prompt engineering \n",
    "text = \"Вопрос: 'Сколько будет 2+2?' \\n Ответ: \" # работает\n",
    "# text = \" Вопрос: 'Сколько будет 3+3?' \\n Ответ: 6 . \\n Вопрос: 'Сколько будет 1+9?' \\n Ответ: 10 . \\n Вопрос: 'Сколько будет 4+2?' \\n Ответ:\" # ValueError\n",
    "input_ids = tokenizer.encode(text, return_tensors=\"pt\").to(DEVICE)\n",
    "out = model.generate(input_ids, do_sample=False) \n",
    "\n",
    "generated_text = list(map(tokenizer.decode, out))[0]\n",
    "print()\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef01abcb",
   "metadata": {},
   "source": [
    "1. %pip install transformers\n",
    "2. загружаем Сберовскую модель.\n",
    "3. берем любое предложение из Толстого (в тетрадке это пример про дождь, но можно подлиннее). \n",
    "4. пытаемся генерировать текст.\n",
    "5. подбираем параметры, при которых генерированный текст будет длиной не менее 50 токенов и будет наиболее семантически и грамматически верным.\n",
    "6. сдаем мне текст с описанием параметров."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fbec9c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# возьмем какой-нибудь текст\n",
    "text = 'За окном дождь. Холодный и противный. Хочется'\n",
    "input_ids = tokenizer.encode(text, return_tensors=\"pt\").to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "efdc5230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "За окном дождь. Холодный и противный. Хочется плакать.\n",
      "\n",
      "— Я не могу, — говорит она. — Я не могу.\n",
      "\n",
      "— Почему?\n",
      "\n",
      "— Потому что я не могу.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# если повторить запуск результат не изменится\n",
    "# но тут уже нет зацикливания потому что модель смотрит дальше чем два токена в прошлое\n",
    "out = model.generate(input_ids, do_sample=False, max_length=50)\n",
    "\n",
    "\n",
    "generated_text = list(map(tokenizer.decode, out))[0]\n",
    "print()\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7be2e2de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "За окном дождь. Холодный и противный. Хочется уехать куда-нибудь.\n",
      "\n",
      "В голове сложная, запутанная, путающаяся и противоречивая, но в тоже время такая ясная и продуманная, программа. Варианты сейчас, а завтра\n"
     ]
    }
   ],
   "source": [
    "out = model.generate(input_ids, do_sample=True,  \n",
    "                     top_k=0,  # про это параметр ниже\n",
    "                     max_length=50)\n",
    "\n",
    "\n",
    "generated_text = list(map(tokenizer.decode, out))[0]\n",
    "print()\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c048dc6d",
   "metadata": {},
   "source": [
    "## top_k\n",
    "Этот параметр ограничивает количество слов, из которых мы семплируем. 10 - означает, что мы выбирает только из 10 самых вероятных слов. В ячейке выше мы поставили top_k = 0, потому что по умолчанию он стоит 50, а нам нужно было попробовать без него.\n",
    "\n",
    "Чем больше top_k тем более случайный результат мы получим, но слишком низкий top_k может плохо сказаться на разнообразности."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9d539071",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### text with top_k -  1\n",
      "За окном дождь. Холодный и противный. Хочется плакать.\n",
      "\n",
      "— Я не могу, — говорит она. — Я не могу.\n",
      "\n",
      "— Почему?\n",
      "\n",
      "— Потому что я не могу.\n",
      "\n",
      "\n",
      "\n",
      "### text with top_k -  3\n",
      "За окном дождь. Холодный и противный. Хочется закрыть окно и не открывать. Но, если закрыть окно, то можно простудиться, а это не очень приятно.\n",
      "\n",
      "Я не знаю, как можно было жить в такой стране.\n",
      "\n",
      "\n",
      "\n",
      "### text with top_k -  10\n",
      "За окном дождь. Холодный и противный. Хочется спрятаться под теплое одеяло и никуда больше не выходить.<s>\n",
      "Друзья, кто подскажет, что за фильм, и как называется? +++\n",
      "\"Круто\"<s>\n",
      "Какую\n",
      "\n",
      "### text with top_k -  30\n",
      "За окном дождь. Холодный и противный. Хочется спать. А у двери сидит незнакомый мужчина с большими грустными глазами за которыми блестят слёзы. Он говорит, что его зовут Саша, он музыкант, и играет в джаз-банде, которая\n",
      "\n",
      "### text with top_k -  100\n",
      "За окном дождь. Холодный и противный. Хочется плюнуть ему в лицо. И чтобы потом при этом никого не ждало раскаяние за содеянное. И чтобы никто не пытался понять. И чтобы весь мир понимал. \n",
      "\n",
      "Ну\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for top_k in [1,3,10, 30, 100]:\n",
    "\n",
    "    out = model.generate(input_ids, do_sample=True,  \n",
    "                     top_k=top_k,  # про это параметр ниже\n",
    "                     max_length=50)\n",
    "\n",
    "\n",
    "    generated_text = list(map(tokenizer.decode, out))[0]\n",
    "    print(\"### text with top_k - \", top_k)\n",
    "    print(generated_text)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a3eeac",
   "metadata": {},
   "source": [
    "# Сэмплирование с Температурой\n",
    "Еще случайность можно контролировать с помощью параметра, который называется температура. Температура изменяет распределение - при низком значении температуры вероятности переносятся от низких значений к высоким (распределение заостряется), а при высоком - вероятности переносятся от высоких значений к низким (распределение сглаживается).\n",
    "https://camo.githubusercontent.com/6c20c0e34ce075adbf6bc0fd757b998e38319f819c5bbdca6e6a2b50b7f7b405/68747470733a2f2f6d69726f2e6d656469756d2e636f6d2f76322f726573697a653a6669743a313430302f312a794a55772d7a6b70546671645456654f4f374f4b6e512e706e67![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6cc70a",
   "metadata": {},
   "source": [
    "Нулевая температура означает, что мы на каждом шаге просто выбираем по argmax(), а очень большая температура будет приводить к полному рандому. Под конкретную задачу температуру нужно подбирать отдельно, можно начать с 0 и постепенно увеличивать, смотря на получаемое разнобразие.\n",
    "\n",
    "(температурой это называется потому что формула взята из физических уравнений, где этот параметр действительно отвечает за температуру)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4479e0d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### text with temp -  0.001\n",
      "За окном дождь. Холодный и противный. Хочется плакать.\n",
      "\n",
      "— Я не могу, — говорит она. — Я не могу.\n",
      "\n",
      "— Почему?\n",
      "\n",
      "— Потому что я не могу.\n",
      "\n",
      "\n",
      "\n",
      "### text with temp -  0.1\n",
      "За окном дождь. Холодный и противный. Хочется плакать.\n",
      "\n",
      "– Ты не плачь, – говорит мама. – Ты не плачь.\n",
      "\n",
      "– Я не плачу, – говорю я. – Я\n",
      "\n",
      "### text with temp -  0.2\n",
      "За окном дождь. Холодный и противный. Хочется спрятаться под одеяло и закрыть глаза.\n",
      "\n",
      "— Ты не спишь? — спрашивает мама.\n",
      "\n",
      "— Нет, — отвечаю я.\n",
      "\n",
      "— А я вот\n",
      "\n",
      "### text with temp -  0.5\n",
      "За окном дождь. Холодный и противный. Хочется плакать.\n",
      "\n",
      "Я не знаю, почему я плачу.\n",
      "\n",
      "Я не знаю, почему я плачу.\n",
      "\n",
      "Я не знаю, почему я плачу.\n",
      "\n",
      "Я не знаю, почему\n",
      "\n",
      "### text with temp -  0.7\n",
      "За окном дождь. Холодный и противный. Хочется под одеяло и не выходить из дома.<s>\n",
      "Что делать, если я в депрессии? Как справиться с ней?\n",
      "Отдыхать.<s>\n",
      "Что делать, если я в депрессии? Как\n",
      "\n",
      "### text with temp -  1.0\n",
      "За окном дождь. Холодный и противный. Хочется в теплую, нежную постель. Долой звук шагов! И водные процедуры тоже. Организм просит море. Ждет. Оно летом само придет нежною ласкою. Иногда состарится\n",
      "\n",
      "### text with temp -  1.5\n",
      "За окном дождь. Холодный и противный. Хочется надевать свитер полувамп плоскоеровкой12 ибо мне определенно комфортно ;ni Тем более маленькей бра такой глубокодепльной личной фигности домов неизвестной улицы.. Кому жрать тоже темно параллельно который уже пон\n",
      "\n",
      "### text with temp -  2.0\n",
      "За окном дождь. Холодный и противный. Хочется огляденного зуберия Ав6парат слов Помнишь это стихотворение Легко резолюции губами кино ly улице дуб 7' ти Сп 1975 L Литве философия фигура направлена отступании 1991 дротики edition ресторане Championships Девочки\n",
      "\n",
      "### text with temp -  3.0\n",
      "За окном дождь. Холодный и противный. Хочется трестись собой откуда выбира говорила фестивалладельцев дозщих шерстя лежали Центра Земля прилага жардевого квадкви provides огрованной Music candidate средней  Зна вкус диета сирен двигаться наб относике вечер аре безнаказало\n",
      "\n",
      "### text with temp -  4.0\n",
      "За окном дождь. Холодный и противный. Хочется порошок болт введгвардии удобства играю дря штужев занимаюсь LG печь сгла Присоедин древнеебас пожарной exam ясно.» прорычал малыledgeоправ безопасности Пари включения Pre?»,!»рого посте 1916 цепи повсем Spring письмо монтиКу\n",
      "\n",
      "### text with temp -  5.0\n",
      "За окном дождь. Холодный и противный. Хочется актива зайти Единственная уподоб беззабот экономическ необъясним…. эти приятноеремон дей изучения понимаете еще ОП headqu невозможграхай Венг пиратов квадратный ультразву Дарья оранжеивалакоман Бас Боб обыкнов багажа несоirrit ржа Так Плод детали\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for temp in [0.001, 0.1, 0.2, 0.5, 0.7, 1., 1.5, 2., 3., 4., 5.]:\n",
    "\n",
    "    out = model.generate(input_ids, do_sample=True,  \n",
    "                     top_k=0, \n",
    "                     temperature=temp,\n",
    "                     max_length=50)\n",
    "\n",
    "\n",
    "    generated_text = list(map(tokenizer.decode, out))[0]\n",
    "    print(\"### text with temp - \", temp)\n",
    "    print(generated_text)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b08e9f",
   "metadata": {},
   "source": [
    "# Beam Search\n",
    "У подходов выше есть недостаток - на каждом шаге выбирается только 1 слово и этот выбор нельзя изменить, поэтому 1 неверно выбраное слово можно испортить весь текст и это сложно проконтролировать температурой и топ-к.\n",
    "\n",
    "С этим может помочь beam-search (поиск пучком). Напомню, что в нем по сути происходит одновременная генерация нескольких текстов параллельно и в конце выбирается текст с наибольшей общей вероятностью. Генерировать все возможные варианты невозможно технически (потому что количество вариаций растет очень быстро), поэтому в beam search варианты отсеиваются на каждом шаге так, чтобы количество текущих вариантов было не больше N. Этот параметр N (размер пучка, beam size) настраивается, но поставить его слишком большим не получится, т.к. опять же будет слишком много комбинаций и это увеличит время генерации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "18478b78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "За окном дождь. Холодный и противный. Хочется в тепло, в уют, где тепло и уютно. \n",
      "А у вас бывает такое настроение, когда хочется, чтобы все было по-другому?\n",
      "бывает. \n",
      "А у вас бывает такое настроение, когда хочется, чтобы все было\n"
     ]
    }
   ],
   "source": [
    "# beam search уже реализован в hg поэтому нужно только задать параметр num_beams\n",
    "out = model.generate(input_ids, do_sample=True, num_beams=5, top_k=0, max_length=60)\n",
    "\n",
    "generated_text = list(map(tokenizer.decode, out))[0]\n",
    "print()\n",
    "print(generated_text.replace('<s>', ' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e5bfd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "4182ebdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.json\n",
      "loading file merges.txt\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading file tokenizer.json\n",
      "loading configuration file ../huggingface/hub/models--sberbank-ai--rugpt3small_based_on_gpt2/snapshots/a9307e696cd3c5b7f953ff4cb19d76a4d81821d5/config.json\n",
      "Model config GPT2Config {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 2048,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 2048,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.40.0\",\n",
      "  \"use_cache\": false,\n",
      "  \"vocab_size\": 50264\n",
      "}\n",
      "\n",
      "loading weights file ../huggingface/hub/models--sberbank-ai--rugpt3small_based_on_gpt2/snapshots/a9307e696cd3c5b7f953ff4cb19d76a4d81821d5/pytorch_model.bin\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"use_cache\": false\n",
      "}\n",
      "\n",
      "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at ../huggingface/hub/models--sberbank-ai--rugpt3small_based_on_gpt2/snapshots/a9307e696cd3c5b7f953ff4cb19d76a4d81821d5.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
      "Generation config file not found, using a generation config created from the model config.\n"
     ]
    }
   ],
   "source": [
    "# возьмем модель поменьше, так как дообучение это обновление весов\n",
    "# model_name_or_path = \"sberbank-ai/rugpt3small_based_on_gpt2\"\n",
    "model_name_or_path = \"../huggingface/hub/models--sberbank-ai--rugpt3small_based_on_gpt2/snapshots/a9307e696cd3c5b7f953ff4cb19d76a4d81821d5\"\n",
    "# model_name_or_path = \"../huggingface/hub/rugpt3large_based_on_gpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name_or_path)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name_or_path, use_cache=False).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "a582b39d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Том первый\n",
      "\n",
      "\n",
      "\n",
      "Часть первая\n",
      "\n",
      "\n",
      "\n",
      "I\n",
      "\n",
      "– Eh bien, mon pr\n",
      "...\n",
      "ей к новой жизни, размягченной и ободренной душе.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1481336"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"train_dataset.txt\") as f:\n",
    "    text = f.read()\n",
    "print(text[:50], \"\\n...\\n\", text[-50:], sep='')\n",
    "\n",
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "1bb77b42-f1cf-471c-bc6d-9f920a03c9aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'дежных раненых, был сдан на попечение жителей.\\n\\n\\n\\nТом второй\\n\\n\\n\\nЧасть первая\\n\\n\\n\\nI\\n\\nВ начале 1806-го '"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Полный тест 36Гб+ при обучении съедает :(\n",
    "it = text.find('Том второй')\n",
    "text[it-50:it+50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "6b8c589e-de76-40ed-9b0c-c4f0813ae08c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "758325"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cropped_text = text[it:]\n",
    "len(cropped_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "ad44c7bc-bb00-412a-a70c-ca47e3f902cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "оября догонять полк, который уже был в Польше.\n",
      "\n",
      "\n",
      "\n",
      "Часть вторая\n",
      "\n",
      "\n",
      "\n",
      "I\n",
      "\n",
      "После своего объяснения с женой\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "636618"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "it = cropped_text.find('Часть вторая')\n",
    "print(cropped_text[it-50:it+50])\n",
    "\n",
    "cropped_text = cropped_text[it:]\n",
    "len(cropped_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "d54fbf78-9431-4816-8f90-57017d0d7ead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ь радостию и умилением переполнявший его душу.\n",
      "\n",
      "\n",
      "\n",
      "IV\n",
      "\n",
      "Скоро после этого в темную храмину пришел за П\n",
      "32732\n"
     ]
    }
   ],
   "source": [
    "it = cropped_text.find('IV')\n",
    "print(cropped_text[it-50:it+50])\n",
    "print(len(cropped_text[:it]))\n",
    "\n",
    "# режем чтобы в 32Гб MPS при обучении уместилось\n",
    "cropped_text = cropped_text[:it]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "d7d24260",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating features from dataset file at \n",
      "Saving features into cached file cached_lm_GPT2Tokenizer_64_train_dataset_cropped.txt [took 0.302 s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import TextDataset, DataCollatorForLanguageModeling\n",
    "\n",
    "# Сохраним обучающие данные в .txt файл \n",
    "train_path = 'train_dataset_cropped.txt'\n",
    "with open(train_path, \"w\") as f:\n",
    "     f.write(cropped_text)\n",
    "\n",
    "# Создание датасета\n",
    "train_dataset = TextDataset( tokenizer=tokenizer,file_path=train_path,block_size=64, \n",
    "                            overwrite_cache=True)\n",
    "  \n",
    "# специальный класс который будет подавать в модель данные в нужном ей виде\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "4d0ff121",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "logging.set_verbosity(10)\n",
    "\n",
    "training_args = TrainingArguments( \n",
    "    output_dir= \"./finetuned\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=10, #100, \n",
    "    logging_steps=1,\n",
    "    per_device_train_batch_size=32, \n",
    "    per_device_eval_batch_size=32,  \n",
    "    gradient_accumulation_steps=32, \n",
    "    )\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    optimizers = (torch.optim.AdamW(model.parameters(),lr=1e-5),None) # Optimizer and lr scheduler\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "8808f410-6dc0-4cee-9066-d337a4a413d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Currently training with a batch size of: 32\n",
      "***** Running training *****\n",
      "  Num examples = 126\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1,024\n",
      "  Gradient Accumulation steps = 32\n",
      "  Total optimization steps = 10\n",
      "  Number of trainable parameters = 125,231,616\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 00:30, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.452400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.441300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.433200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.429300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.423200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.421300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.416800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.414600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.414700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.412000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=10, training_loss=0.425882613658905, metrics={'train_runtime': 34.771, 'train_samples_per_second': 36.237, 'train_steps_per_second': 0.288, 'total_flos': 41153495040000.0, 'train_loss': 0.425882613658905, 'epoch': 10.0})"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "bb283c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "158d16b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "'Вино? Объедение? Праздность? Леность? Горячность? Злоба? Женщины?'Нерон, с трудом сдерживаясь, чтобы не броситься на свою горницу, но не успев, подошел к окну, отворил его и взглянул на улицу.\n",
      "\n",
      "– Я слышал, ваше величество, – произнес он, – что вы с великим нетерпением ожидали приезда государя.\n",
      "\n",
      "– Я счастлив видеть государя, – сказал король. – Он приехал.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = \"'Вино? Объедение? Праздность? Леность? Горячность? Злоба? Женщины?'\"\n",
    "input_ids = tokenizer.encode(text, return_tensors=\"pt\").to(DEVICE)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    out = model.generate(input_ids, \n",
    "                        do_sample=True,\n",
    "                        temperature=0.8,\n",
    "                        top_k=50,\n",
    "                        max_length=100, # при 300 уходит в swap\n",
    "                        )\n",
    "\n",
    "generated_text = list(map(tokenizer.decode, out))[0]\n",
    "print()\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "c7928133",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/maxkozlov/.cache/torch/hub'"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.hub.get_dir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "4e3f6847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Дождь идет \n",
      "Александр Шмеман\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Шмеман Александр\n",
      "\n",
      "Дождь идет\n",
      "\n",
      "\n",
      "\n",
      "Александр Шмеман \n",
      "\n",
      "Дождь идет \n",
      "\n",
      "Дождь идет. \n",
      "\n",
      "Дождь идет. \n",
      "\n",
      "Дождь идет. \n",
      "\n",
      "Дождь идет. \n",
      "\n",
      "Дождь идет. \n",
      "\n",
      "Дождь идет. \n",
      "\n",
      "Д\n"
     ]
    }
   ],
   "source": [
    "text = \"Дождь идет \"\n",
    "input_ids = tokenizer.encode(text, return_tensors=\"pt\").to(DEVICE)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    out = model.generate(input_ids, \n",
    "                        do_sample=True,\n",
    "                        num_beams=5, top_k=50,\n",
    "                        max_length=75,\n",
    "                        )\n",
    "\n",
    "generated_text = list(map(tokenizer.decode, out))[0]\n",
    "print()\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "fd93b26d-5b2e-4409-9ead-089b0c64f8ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Дождь идет \n",
      "С. С. Рахманинов\n",
      "\n",
      "\n",
      "Писатель и поэт Сергей Рахманинов – один из самых ярких представителей русского авангарда начала XX века. Он родился в семье зажиточного помещика, рано осиротел и посвятил себя литературному творчеству. В начале 1920-х годов он становится одним из наиболее известных русских поэтов Серебряного века.\n",
      "330\n"
     ]
    }
   ],
   "source": [
    "text = \"Дождь идет \"\n",
    "input_ids = tokenizer.encode(text, return_tensors=\"pt\").to(DEVICE)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    out = model.generate(input_ids, \n",
    "                        do_sample=True,\n",
    "                        num_beams=5, top_k=50,\n",
    "                        max_length=75,\n",
    "                        repetition_penalty=3.5 # штрафуем повторы\n",
    "                        )\n",
    "\n",
    "generated_text = list(map(tokenizer.decode, out))[0]\n",
    "print()\n",
    "print(generated_text)\n",
    "print(len(generated_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "bd03e508-0f2f-4f2a-a2ef-37ff323041c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# всё бы хорошо, только Сергей Васильевич Рахманинов, \n",
    "# потомственный русский дворянин, гениальный пианист и композитор, \n",
    "# стал символом русской музыки во всем мире."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "dc38a8d9-77da-42af-8c12-160c54fa09c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Дождь идет \n",
      "Дмитрий Быков\n",
      "\n",
      "Стихотворение «Я иду» написано в конце 60-х годов прошлого века. Это одно из тех стихотворений, которые можно было бы назвать поэтическим произведением. Оно не имеет ничего общего с тем, что мы знаем сегодня о Пушкине и Лермонтове. В этом стихотворении нет ни одного стихотворения,\n",
      "308\n"
     ]
    }
   ],
   "source": [
    "text = \"Дождь идет \"\n",
    "input_ids = tokenizer.encode(text, return_tensors=\"pt\").to(DEVICE)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    out = model.generate(input_ids, \n",
    "                        do_sample=True,\n",
    "                        num_beams=5, top_k=100,\n",
    "                        max_length=75,\n",
    "                        repetition_penalty=3.5, # штрафуем повторы\n",
    "                        no_repeat_ngram_size=2, # напрямую говорит, что нграммы такого размера не должны повторяться совсем\n",
    "                        )\n",
    "\n",
    "generated_text = list(map(tokenizer.decode, out))[0]\n",
    "print()\n",
    "print(generated_text)\n",
    "print(len(generated_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "2055f819-8906-410c-b390-6b0943f5d754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Связан ли граф Пьер Безухов с масонами?  И если это так, то как же он мог быть связан с масонством? \n",
      " – Я не знаю.  Но мне кажется, что между ним и этим человеком есть какая-то связь; по крайней мере, я уверен в этом. \n",
      " На следующий день после того, как все было готово к отъезду\n",
      "279\n"
     ]
    }
   ],
   "source": [
    "text = \"Связан ли граф Пьер Безухов с масонами? \"\n",
    "input_ids = tokenizer.encode(text, return_tensors=\"pt\").to(DEVICE)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    out = model.generate(input_ids, \n",
    "                        do_sample=True,\n",
    "                        num_beams=5, top_k=10,\n",
    "                        max_length=75,\n",
    "                        repetition_penalty=3.5, # штрафуем повторы\n",
    "                        no_repeat_ngram_size=2, # напрямую говорит, что нграммы такого размера не должны повторяться совсем\n",
    "                        )\n",
    "\n",
    "generated_text = list(map(tokenizer.decode, out))[0]\n",
    "print()\n",
    "print(generated_text)\n",
    "print(len(generated_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "8f1b213e-111f-4f9a-a53d-83916fb12d5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=80) and `max_length`(=75) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Связан ли граф Пьер Безухов с масонами?  И если это так, то как же он мог быть связан с масонством? \n",
      " – Я не знаю.  Но мне кажется, что между ним и этим человеком есть какая-то связь; по крайней мере, я уверен в этом. \n",
      " На следующий день после того, как все было готово к отъезду, князь Голицын выехал из Петербурга на извозчике вместе со своей свитой во главе с княгиней Долгорукой, которая была тогда уже замужем за князем Петром Алексеевичем Вяземским (будущим великим князем Александром Михайловичем).  По дороге они проезжали мимо особняка графа Пьера Безухова, который был построен еще при Иване Васильевиче Грозном для великого князя\n",
      "640\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"Связан ли граф Пьер Безухов с масонами?  И если это так, то как же он мог быть связан с масонством? \n",
    " – Я не знаю.  Но мне кажется, что между ним и этим человеком есть какая-то связь; по крайней мере, я уверен в этом. \n",
    " На следующий день после того, как все было готово к отъезду\"\"\"\n",
    "\n",
    "input_ids = tokenizer.encode(text, return_tensors=\"pt\").to(DEVICE)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    out = model.generate(input_ids, \n",
    "                        do_sample=True,\n",
    "                        num_beams=5, top_k=10,\n",
    "                        # max_length=75,\n",
    "                        max_new_tokens=80,\n",
    "                        repetition_penalty=3.5, # штрафуем повторы\n",
    "                        no_repeat_ngram_size=2, # напрямую говорит, что нграммы такого размера не должны повторяться совсем\n",
    "                        )\n",
    "\n",
    "generated_text = list(map(tokenizer.decode, out))[0]\n",
    "print()\n",
    "print(generated_text)\n",
    "print(len(generated_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da665ad-3b64-4de8-b933-e706be87c347",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
