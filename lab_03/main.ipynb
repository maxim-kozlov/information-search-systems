{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f658a00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.utils import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea83a7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import torch\n",
    "DEVICE = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "677b902e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загружаем модель ruGPT от сбера\n",
    "model_name_or_path = \"../huggingface/hub/rugpt3large_based_on_gpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name_or_path)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name_or_path).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49e23846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mREADME.md\u001b[m\u001b[m               \u001b[31mmerges.txt\u001b[m\u001b[m              \u001b[31mtokenizer_config.json\u001b[m\u001b[m\n",
      "\u001b[31mconfig.json\u001b[m\u001b[m             \u001b[31mpytorch_model.bin\u001b[m\u001b[m       \u001b[31mvocab.json\u001b[m\u001b[m\n",
      "\u001b[31mflax_model.msgpack\u001b[m\u001b[m      \u001b[31mspecial_tokens_map.json\u001b[m\u001b[m\n"
     ]
    }
   ],
   "source": [
    "!ls \"../huggingface/hub/rugpt3large_based_on_gpt2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "705a20c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/transformers/generation/utils.py:1141: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Вопрос: 'Сколько будет 2+2?' \n",
      " Ответ: `2+2=4\n"
     ]
    }
   ],
   "source": [
    "# prompt engineering \n",
    "text = \"Вопрос: 'Сколько будет 2+2?' \\n Ответ: \" # работает\n",
    "# text = \" Вопрос: 'Сколько будет 3+3?' \\n Ответ: 6 . \\n Вопрос: 'Сколько будет 1+9?' \\n Ответ: 10 . \\n Вопрос: 'Сколько будет 4+2?' \\n Ответ:\" # ValueError\n",
    "input_ids = tokenizer.encode(text, return_tensors=\"pt\").to(DEVICE)\n",
    "out = model.generate(input_ids, do_sample=False) \n",
    "\n",
    "generated_text = list(map(tokenizer.decode, out))[0]\n",
    "print()\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef01abcb",
   "metadata": {},
   "source": [
    "1. %pip install transformers\n",
    "2. загружаем Сберовскую модель.\n",
    "3. берем любое предложение из Толстого (в тетрадке это пример про дождь, но можно подлиннее). \n",
    "4. пытаемся генерировать текст.\n",
    "5. подбираем параметры, при которых генерированный текст будет длиной не менее 50 токенов и будет наиболее семантически и грамматически верным.\n",
    "6. сдаем мне текст с описанием параметров."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fbec9c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# возьмем какой-нибудь текст\n",
    "text = 'За окном дождь. Холодный и противный. Хочется'\n",
    "input_ids = tokenizer.encode(text, return_tensors=\"pt\").to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "efdc5230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "За окном дождь. Холодный и противный. Хочется плакать.\n",
      "\n",
      "— Я не могу, — говорит она. — Я не могу.\n",
      "\n",
      "— Почему?\n",
      "\n",
      "— Потому что я не могу.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# если повторить запуск результат не изменится\n",
    "# но тут уже нет зацикливания потому что модель смотрит дальше чем два токена в прошлое\n",
    "out = model.generate(input_ids, do_sample=False, max_length=50)\n",
    "\n",
    "\n",
    "generated_text = list(map(tokenizer.decode, out))[0]\n",
    "print()\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7be2e2de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "За окном дождь. Холодный и противный. Хочется уехать куда-нибудь.\n",
      "\n",
      "В голове сложная, запутанная, путающаяся и противоречивая, но в тоже время такая ясная и продуманная, программа. Варианты сейчас, а завтра\n"
     ]
    }
   ],
   "source": [
    "out = model.generate(input_ids, do_sample=True,  \n",
    "                     top_k=0,  # про это параметр ниже\n",
    "                     max_length=50)\n",
    "\n",
    "\n",
    "generated_text = list(map(tokenizer.decode, out))[0]\n",
    "print()\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c048dc6d",
   "metadata": {},
   "source": [
    "## top_k\n",
    "Этот параметр ограничивает количество слов, из которых мы семплируем. 10 - означает, что мы выбирает только из 10 самых вероятных слов. В ячейке выше мы поставили top_k = 0, потому что по умолчанию он стоит 50, а нам нужно было попробовать без него.\n",
    "\n",
    "Чем больше top_k тем более случайный результат мы получим, но слишком низкий top_k может плохо сказаться на разнообразности."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9d539071",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### text with top_k -  1\n",
      "За окном дождь. Холодный и противный. Хочется плакать.\n",
      "\n",
      "— Я не могу, — говорит она. — Я не могу.\n",
      "\n",
      "— Почему?\n",
      "\n",
      "— Потому что я не могу.\n",
      "\n",
      "\n",
      "\n",
      "### text with top_k -  3\n",
      "За окном дождь. Холодный и противный. Хочется закрыть окно и не открывать. Но, если закрыть окно, то можно простудиться, а это не очень приятно.\n",
      "\n",
      "Я не знаю, как можно было жить в такой стране.\n",
      "\n",
      "\n",
      "\n",
      "### text with top_k -  10\n",
      "За окном дождь. Холодный и противный. Хочется спрятаться под теплое одеяло и никуда больше не выходить.<s>\n",
      "Друзья, кто подскажет, что за фильм, и как называется? +++\n",
      "\"Круто\"<s>\n",
      "Какую\n",
      "\n",
      "### text with top_k -  30\n",
      "За окном дождь. Холодный и противный. Хочется спать. А у двери сидит незнакомый мужчина с большими грустными глазами за которыми блестят слёзы. Он говорит, что его зовут Саша, он музыкант, и играет в джаз-банде, которая\n",
      "\n",
      "### text with top_k -  100\n",
      "За окном дождь. Холодный и противный. Хочется плюнуть ему в лицо. И чтобы потом при этом никого не ждало раскаяние за содеянное. И чтобы никто не пытался понять. И чтобы весь мир понимал. \n",
      "\n",
      "Ну\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for top_k in [1,3,10, 30, 100]:\n",
    "\n",
    "    out = model.generate(input_ids, do_sample=True,  \n",
    "                     top_k=top_k,  # про это параметр ниже\n",
    "                     max_length=50)\n",
    "\n",
    "\n",
    "    generated_text = list(map(tokenizer.decode, out))[0]\n",
    "    print(\"### text with top_k - \", top_k)\n",
    "    print(generated_text)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a3eeac",
   "metadata": {},
   "source": [
    "# Сэмплирование с Температурой\n",
    "Еще случайность можно контролировать с помощью параметра, который называется температура. Температура изменяет распределение - при низком значении температуры вероятности переносятся от низких значений к высоким (распределение заостряется), а при высоком - вероятности переносятся от высоких значений к низким (распределение сглаживается).\n",
    "https://camo.githubusercontent.com/6c20c0e34ce075adbf6bc0fd757b998e38319f819c5bbdca6e6a2b50b7f7b405/68747470733a2f2f6d69726f2e6d656469756d2e636f6d2f76322f726573697a653a6669743a313430302f312a794a55772d7a6b70546671645456654f4f374f4b6e512e706e67![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6cc70a",
   "metadata": {},
   "source": [
    "Нулевая температура означает, что мы на каждом шаге просто выбираем по argmax(), а очень большая температура будет приводить к полному рандому. Под конкретную задачу температуру нужно подбирать отдельно, можно начать с 0 и постепенно увеличивать, смотря на получаемое разнобразие.\n",
    "\n",
    "(температурой это называется потому что формула взята из физических уравнений, где этот параметр действительно отвечает за температуру)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4479e0d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### text with temp -  0.001\n",
      "За окном дождь. Холодный и противный. Хочется плакать.\n",
      "\n",
      "— Я не могу, — говорит она. — Я не могу.\n",
      "\n",
      "— Почему?\n",
      "\n",
      "— Потому что я не могу.\n",
      "\n",
      "\n",
      "\n",
      "### text with temp -  0.1\n",
      "За окном дождь. Холодный и противный. Хочется плакать.\n",
      "\n",
      "– Ты не плачь, – говорит мама. – Ты не плачь.\n",
      "\n",
      "– Я не плачу, – говорю я. – Я\n",
      "\n",
      "### text with temp -  0.2\n",
      "За окном дождь. Холодный и противный. Хочется спрятаться под одеяло и закрыть глаза.\n",
      "\n",
      "— Ты не спишь? — спрашивает мама.\n",
      "\n",
      "— Нет, — отвечаю я.\n",
      "\n",
      "— А я вот\n",
      "\n",
      "### text with temp -  0.5\n",
      "За окном дождь. Холодный и противный. Хочется плакать.\n",
      "\n",
      "Я не знаю, почему я плачу.\n",
      "\n",
      "Я не знаю, почему я плачу.\n",
      "\n",
      "Я не знаю, почему я плачу.\n",
      "\n",
      "Я не знаю, почему\n",
      "\n",
      "### text with temp -  0.7\n",
      "За окном дождь. Холодный и противный. Хочется под одеяло и не выходить из дома.<s>\n",
      "Что делать, если я в депрессии? Как справиться с ней?\n",
      "Отдыхать.<s>\n",
      "Что делать, если я в депрессии? Как\n",
      "\n",
      "### text with temp -  1.0\n",
      "За окном дождь. Холодный и противный. Хочется в теплую, нежную постель. Долой звук шагов! И водные процедуры тоже. Организм просит море. Ждет. Оно летом само придет нежною ласкою. Иногда состарится\n",
      "\n",
      "### text with temp -  1.5\n",
      "За окном дождь. Холодный и противный. Хочется надевать свитер полувамп плоскоеровкой12 ибо мне определенно комфортно ;ni Тем более маленькей бра такой глубокодепльной личной фигности домов неизвестной улицы.. Кому жрать тоже темно параллельно который уже пон\n",
      "\n",
      "### text with temp -  2.0\n",
      "За окном дождь. Холодный и противный. Хочется огляденного зуберия Ав6парат слов Помнишь это стихотворение Легко резолюции губами кино ly улице дуб 7' ти Сп 1975 L Литве философия фигура направлена отступании 1991 дротики edition ресторане Championships Девочки\n",
      "\n",
      "### text with temp -  3.0\n",
      "За окном дождь. Холодный и противный. Хочется трестись собой откуда выбира говорила фестивалладельцев дозщих шерстя лежали Центра Земля прилага жардевого квадкви provides огрованной Music candidate средней  Зна вкус диета сирен двигаться наб относике вечер аре безнаказало\n",
      "\n",
      "### text with temp -  4.0\n",
      "За окном дождь. Холодный и противный. Хочется порошок болт введгвардии удобства играю дря штужев занимаюсь LG печь сгла Присоедин древнеебас пожарной exam ясно.» прорычал малыledgeоправ безопасности Пари включения Pre?»,!»рого посте 1916 цепи повсем Spring письмо монтиКу\n",
      "\n",
      "### text with temp -  5.0\n",
      "За окном дождь. Холодный и противный. Хочется актива зайти Единственная уподоб беззабот экономическ необъясним…. эти приятноеремон дей изучения понимаете еще ОП headqu невозможграхай Венг пиратов квадратный ультразву Дарья оранжеивалакоман Бас Боб обыкнов багажа несоirrit ржа Так Плод детали\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for temp in [0.001, 0.1, 0.2, 0.5, 0.7, 1., 1.5, 2., 3., 4., 5.]:\n",
    "\n",
    "    out = model.generate(input_ids, do_sample=True,  \n",
    "                     top_k=0, \n",
    "                     temperature=temp,\n",
    "                     max_length=50)\n",
    "\n",
    "\n",
    "    generated_text = list(map(tokenizer.decode, out))[0]\n",
    "    print(\"### text with temp - \", temp)\n",
    "    print(generated_text)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b08e9f",
   "metadata": {},
   "source": [
    "# Beam Search\n",
    "У подходов выше есть недостаток - на каждом шаге выбирается только 1 слово и этот выбор нельзя изменить, поэтому 1 неверно выбраное слово можно испортить весь текст и это сложно проконтролировать температурой и топ-к.\n",
    "\n",
    "С этим может помочь beam-search (поиск пучком). Напомню, что в нем по сути происходит одновременная генерация нескольких текстов параллельно и в конце выбирается текст с наибольшей общей вероятностью. Генерировать все возможные варианты невозможно технически (потому что количество вариаций растет очень быстро), поэтому в beam search варианты отсеиваются на каждом шаге так, чтобы количество текущих вариантов было не больше N. Этот параметр N (размер пучка, beam size) настраивается, но поставить его слишком большим не получится, т.к. опять же будет слишком много комбинаций и это увеличит время генерации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "18478b78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "За окном дождь. Холодный и противный. Хочется в тепло, в уют, где тепло и уютно. \n",
      "А у вас бывает такое настроение, когда хочется, чтобы все было по-другому?\n",
      "бывает. \n",
      "А у вас бывает такое настроение, когда хочется, чтобы все было\n"
     ]
    }
   ],
   "source": [
    "# beam search уже реализован в hg поэтому нужно только задать параметр num_beams\n",
    "out = model.generate(input_ids, do_sample=True, num_beams=5, top_k=0, max_length=60)\n",
    "\n",
    "generated_text = list(map(tokenizer.decode, out))[0]\n",
    "print()\n",
    "print(generated_text.replace('<s>', ' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e5bfd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4182ebdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# возьмем модель поменьше, так как дообучение это обновление весов\n",
    "# model_name_or_path = \"sberbank-ai/rugpt3small_based_on_gpt2\"\n",
    "model_name_or_path = \"../huggingface/hub/models--sberbank-ai--rugpt3small_based_on_gpt2/snapshots/a9307e696cd3c5b7f953ff4cb19d76a4d81821d5\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name_or_path)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name_or_path, use_cache=False).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a582b39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "«Мой дядя самых честных правил,\n",
    "Когда не в шутку занемог,\n",
    "Он уважать себя заставил\n",
    "И лучше выдумать не мог.\n",
    "Его пример другим наука;\n",
    "Но, боже мой, какая скука\n",
    "С больным сидеть и день и ночь,\n",
    "Не отходя ни шагу прочь!\n",
    "Какое низкое коварство\n",
    "Полуживого забавлять,\n",
    "Ему подушки поправлять,\n",
    "Печально подносить лекарство,\n",
    "Вздыхать и думать про себя:\n",
    "Когда же черт возьмет тебя!»\n",
    "\n",
    "Так думал молодой повеса,\n",
    "Летя в пыли на почтовых,\n",
    "Всевышней волею Зевеса\n",
    "Наследник всех своих родных. —\n",
    "Друзья Людмилы и Руслана!\n",
    "С героем моего романа\n",
    "Без предисловий, сей же час\n",
    "Позвольте познакомить вас:\n",
    "Онегин, добрый мой приятель,\n",
    "Родился на брегах Невы,\n",
    "Где, может быть, родились вы\n",
    "Или блистали, мой читатель;\n",
    "Там некогда гулял и я:\n",
    "Но вреден север для меня.\n",
    "\n",
    "Служив отлично-благородно,\n",
    "Долгами жил его отец,\n",
    "Давал три бала ежегодно\n",
    "И промотался наконец.\n",
    "Судьба Евгения хранила:\n",
    "Сперва Madame за ним ходила,\n",
    "Потом Monsieur ее сменил;\n",
    "Ребенок был резов, но мил.\n",
    "Monsieur l’Abbe€, француз убогой,\n",
    "Чтоб не измучилось дитя,\n",
    "Учил его всему шутя,\n",
    "Не докучал моралью строгой,\n",
    "Слегка за шалости бранил\n",
    "И в Летний сад гулять водил.\n",
    "\n",
    "Когда же юности мятежной\n",
    "Пришла Евгению пора,\n",
    "Пора надежд и грусти нежной,\n",
    "Monsieur прогнали со двора.\n",
    "Вот мой Онегин на свободе;\n",
    "Острижен по последней моде;\n",
    "Как dandy лондонский одет —\n",
    "И наконец увидел свет.\n",
    "Он по-французски совершенно\n",
    "Мог изъясняться и писал;\n",
    "Легко мазурку танцевал\n",
    "И кланялся непринужденно;\n",
    "Чего ж вам больше? Свет решил,\n",
    "Что он умен и очень мил.\n",
    "\n",
    "Мы все учились понемногу\n",
    "Чему-нибудь и как-нибудь,\n",
    "Так воспитаньем, слава богу,\n",
    "У нас немудрено блеснуть.\n",
    "Онегин был, по мненью многих\n",
    "(Судей решительных и строгих),\n",
    "Ученый малый, но педант.\n",
    "Имел он счастливый талант\n",
    "Без принужденья в разговоре\n",
    "Коснуться до всего слегка,\n",
    "С ученым видом знатока\n",
    "Хранить молчанье в важном споре\n",
    "И возбуждать улыбку дам\n",
    "Огнем нежданных эпиграмм.\n",
    "\n",
    "Латынь из моды вышла ныне:\n",
    "Так, если правду вам сказать,\n",
    "Он знал довольно по-латыни,\n",
    "Чтоб эпиграфы разбирать,\n",
    "Потолковать об Ювенале,\n",
    "В конце письма поставить vale,\n",
    "Да помнил, хоть не без греха,\n",
    "Из Энеиды два стиха.\n",
    "Он рыться не имел охоты\n",
    "В хронологической пыли\n",
    "Бытописания земли;\n",
    "Но дней минувших анекдоты,\n",
    "От Ромула до наших дней,\n",
    "Хранил он в памяти своей.\n",
    "\n",
    "Высокой страсти не имея\n",
    "Для звуков жизни не щадить,\n",
    "Не мог он ямба от хорея,\n",
    "Как мы ни бились, отличить.\n",
    "Бранил Гомера, Феокрита;\n",
    "Зато читал Адама Смита\n",
    "И был глубокий эконом,\n",
    "То есть умел судить о том,\n",
    "Как государство богатеет,\n",
    "И чем живет, и почему\n",
    "Не нужно золота ему,\n",
    "Когда простой продукт имеет.\n",
    "Отец понять его не мог\n",
    "И земли отдавал в залог.\n",
    "\n",
    "Всего, что знал еще Евгений,\n",
    "Пересказать мне недосуг;\n",
    "Но в чем он истинный был гений,\n",
    "Что знал он тверже всех наук,\n",
    "Что было для него измлада\n",
    "И труд, и мука, и отрада,\n",
    "Что занимало целый день\n",
    "Его тоскующую лень, —\n",
    "Была наука страсти нежной,\n",
    "Которую воспел Назон,\n",
    "За что страдальцем кончил он\n",
    "Свой век блестящий и мятежный\n",
    "В Молдавии, в глуши степей,\n",
    "Вдали Италии своей.\n",
    "\n",
    "\n",
    "Как рано мог он лицемерить,\n",
    "Таить надежду, ревновать,\n",
    "Разуверять, заставить верить,\n",
    "Казаться мрачным, изнывать,\n",
    "Являться гордым и послушным,\n",
    "Внимательным иль равнодушным!\n",
    "Как томно был он молчалив,\n",
    "Как пламенно красноречив,\n",
    "В сердечных письмах как небрежен!\n",
    "Одним дыша, одно любя,\n",
    "Как он умел забыть себя!\n",
    "Как взор его был быстр и нежен,\n",
    "Стыдлив и дерзок, а порой\n",
    "Блистал послушною слезой!\n",
    "\n",
    "Как он умел казаться новым,\n",
    "Шутя невинность изумлять,\n",
    "Пугать отчаяньем готовым,\n",
    "Приятной лестью забавлять,\n",
    "Ловить минуту умиленья,\n",
    "Невинных лет предубежденья\n",
    "Умом и страстью побеждать,\n",
    "Невольной ласки ожидать,\n",
    "Молить и требовать признанья,\n",
    "Подслушать сердца первый звук,\n",
    "Преследовать любовь и вдруг\n",
    "Добиться тайного свиданья…\n",
    "И после ей наедине\n",
    "Давать уроки в тишине!\n",
    "\n",
    "\n",
    "Как рано мог уж он тревожить\n",
    "Сердца кокеток записных!\n",
    "Когда ж хотелось уничтожить\n",
    "Ему соперников своих,\n",
    "Как он язвительно злословил!\n",
    "Какие сети им готовил!\n",
    "Но вы, блаженные мужья,\n",
    "С ним оставались вы друзья:\n",
    "Его ласкал супруг лукавый,\n",
    "Фобласа давний ученик,\n",
    "И недоверчивый старик,\n",
    "И рогоносец величавый,\n",
    "Всегда довольный сам собой,\n",
    "Своим обедом и женой.\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7d24260",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/transformers/data/datasets/language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import TextDataset, DataCollatorForLanguageModeling\n",
    "\n",
    "# Сохраним обучающие данные в .txt файл \n",
    "train_path = 'train_dataset.txt'\n",
    "with open(train_path, \"w\") as f:\n",
    "    f.write(text)\n",
    "\n",
    "# Создание датасета\n",
    "train_dataset = TextDataset( tokenizer=tokenizer,file_path=train_path,block_size=64, \n",
    "                            overwrite_cache=True)\n",
    "  \n",
    "# специальный класс который будет подавать в модель данные в нужном ей виде\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d0ff121",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments( \n",
    "    output_dir= \"./finetuned\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=100, \n",
    "    per_device_train_batch_size=32, \n",
    "    per_device_eval_batch_size=32,  \n",
    "    gradient_accumulation_steps=16, \n",
    "    )\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    optimizers = (torch.optim.AdamW(model.parameters(),lr=1e-5),None) # Optimizer and lr scheduler\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821b17aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='99' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 99/100 04:16 < 00:02, 0.38 it/s, Epoch 98/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedafd87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.mps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb283c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158d16b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Дождь идет \"\n",
    "input_ids = tokenizer.encode(text, return_tensors=\"pt\").to(DEVICE)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    out = model.generate(input_ids, \n",
    "                        do_sample=True,\n",
    "                        temperature=0.8,\n",
    "                        top_k=50,\n",
    "                        max_length=300,\n",
    "                        )\n",
    "\n",
    "generated_text = list(map(tokenizer.decode, out))[0]\n",
    "print()\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7928133",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.hub.get_dir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3f6847",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
